vLLM is an optimized , high throughput and serving engine for LLM.  it uses paged-attention to enable fast and memory efficient interface. 

reference link:
https://github.com/vllm-project/production-stack
https://www.kubeai.org/
https://www.redhat.com/en/topics/ai/what-is-vllm