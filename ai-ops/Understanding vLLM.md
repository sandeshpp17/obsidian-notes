vLLM is an optimized , high throughput and serving engine for LLM.  it uses paged-attention to enable fast and memory efficient interface. 

